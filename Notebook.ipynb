{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json \n",
    "import random\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "from matplotlib import pyplot, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== File content ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'SROIE2019/test/box/X51007846397.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m== File content ==\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mhead -n 5 \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{bbox_file_path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m bbox \u001b[39m=\u001b[39m read_bbox_and_words(Pathh\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSROIE2019/test/box/X51007846397.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m== Dataframe ==\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m bbox\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 18\u001b[0m, in \u001b[0;36mread_bbox_and_words\u001b[1;34m(Pathh)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39m# From the splited line we save (filename, [bounding box points], text line).\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39m# The filename will be useful in the future\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     bbox_and_words_list\u001b[39m.\u001b[39mappend([Pathh, \u001b[39m*\u001b[39mbbox, text])\n\u001b[1;32m---> 18\u001b[0m dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(bbox_and_words_list, columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mfilename\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mx0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39my0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mx1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39my1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mx2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39my2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mx3\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39my3\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mline\u001b[39;49m\u001b[39m'\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mint16)\n\u001b[0;32m     19\u001b[0m dataframe \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my3\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m dataframe\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:790\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    781\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    782\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    783\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m         dtype,\n\u001b[0;32m    789\u001b[0m     )\n\u001b[1;32m--> 790\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    791\u001b[0m         arrays,\n\u001b[0;32m    792\u001b[0m         columns,\n\u001b[0;32m    793\u001b[0m         index,\n\u001b[0;32m    794\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    795\u001b[0m         typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    796\u001b[0m     )\n\u001b[0;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    799\u001b[0m         data,\n\u001b[0;32m    800\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    805\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[0;32m    119\u001b[0m     \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     arrays, refs \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[0;32m    121\u001b[0m     \u001b[39m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[39m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[39m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:607\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    604\u001b[0m         val \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(val)\n\u001b[0;32m    605\u001b[0m     val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_multiget(val, oindex\u001b[39m.\u001b[39m_values, default\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan)\n\u001b[1;32m--> 607\u001b[0m val \u001b[39m=\u001b[39m sanitize_array(val, index, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    608\u001b[0m com\u001b[39m.\u001b[39mrequire_length_match(val, index)\n\u001b[0;32m    609\u001b[0m refs\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:576\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    572\u001b[0m             subarr \u001b[39m=\u001b[39m subarr\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    574\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    575\u001b[0m         \u001b[39m# we will try to copy by-definition here\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m         subarr \u001b[39m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[0;32m    578\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39m__array__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    579\u001b[0m     \u001b[39m# e.g. dask array GH#38645\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:763\u001b[0m, in \u001b[0;36m_try_cast\u001b[1;34m(arr, dtype, copy)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[39m# GH#15832: Check if we are requesting a numeric dtype and\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[39m# that we can convert the data to the requested dtype.\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39melif\u001b[39;00m is_integer_dtype(dtype):\n\u001b[0;32m    761\u001b[0m     \u001b[39m# this will raise if we have e.g. floats\u001b[39;00m\n\u001b[1;32m--> 763\u001b[0m     subarr \u001b[39m=\u001b[39m maybe_cast_to_integer_array(arr, dtype)\n\u001b[0;32m    764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m     subarr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(arr, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1652\u001b[0m, in \u001b[0;36mmaybe_cast_to_integer_array\u001b[1;34m(arr, dtype)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m   1651\u001b[0m             warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, category\u001b[39m=\u001b[39m\u001b[39mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m-> 1652\u001b[0m             casted \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   1653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOverflowError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOverflowError\u001b[39;00m(\n\u001b[0;32m   1655\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe elements provided in the data cannot all be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1656\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcasted to the dtype \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1657\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'SROIE2019/test/box/X51007846397.txt'"
     ]
    }
   ],
   "source": [
    "def read_bbox_and_words(Pathh):\n",
    "  bbox_and_words_list = []\n",
    "\n",
    "  with open(Pathh, 'r', errors='ignore') as f:\n",
    "    for line in f.read().splitlines():\n",
    "      if len(line) == 0:\n",
    "        continue\n",
    "        \n",
    "      split_lines = line.split(\",\")\n",
    "\n",
    "      bbox = np.array(split_lines[0:8], dtype=np.int32)\n",
    "      text = \",\".join(split_lines[8:])\n",
    "\n",
    "      # From the splited line we save (filename, [bounding box points], text line).\n",
    "      # The filename will be useful in the future\n",
    "      bbox_and_words_list.append([Pathh, *bbox, text])\n",
    "    \n",
    "  dataframe = pd.DataFrame(bbox_and_words_list, columns=['filename', 'x0', 'y0', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'line'], dtype=np.int16)\n",
    "  dataframe = dataframe.drop(columns=['x1', 'y1', 'x3', 'y3'])\n",
    "\n",
    "  return dataframe\n",
    "print(\"== File content ==\")\n",
    "!head -n 5 \"{bbox_file_path}\"\n",
    "\n",
    "bbox = read_bbox_and_words(Pathh='SROIE2019/test/box/X51007846397.txt')\n",
    "print(\"\\n== Dataframe ==\")\n",
    "bbox.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== File content ==\n",
      "\n",
      "\n",
      "== Dataframe ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>address</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F&amp;P PHARMACY</td>\n",
       "      <td>02/03/2018</td>\n",
       "      <td>NO.20. GROUND FLOOR, JALAN BS 10/6 TAMAN BUKIT...</td>\n",
       "      <td>31.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        company        date  \\\n",
       "0  F&P PHARMACY  02/03/2018   \n",
       "\n",
       "                                             address  total  \n",
       "0  NO.20. GROUND FLOOR, JALAN BS 10/6 TAMAN BUKIT...  31.90  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_entities(path: Path):\n",
    "  with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "  dataframe = pd.DataFrame([data])\n",
    "  return dataframe\n",
    "\n",
    "\n",
    "# Example usage\n",
    "entities_file_path = sroie_folder_path /  \"test/entities\" / example_file\n",
    "print(\"== File content ==\")\n",
    "!head \"{entities_file_path}\"\n",
    "\n",
    "entities = read_entities(path=entities_file_path)\n",
    "print(\"\\n\\n== Dataframe ==\")\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[39mreturn\u001b[39;00m column\u001b[39m.\u001b[39mupper()\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m line \u001b[39m=\u001b[39m bbox\u001b[39m.\u001b[39mloc[\u001b[39m1\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     24\u001b[0m label \u001b[39m=\u001b[39m assign_line_label(line, entities)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLine:\u001b[39m\u001b[39m\"\u001b[39m, line)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bbox' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign a label to the line by checking the similarity\n",
    "# of the line and all the entities\n",
    "def assign_line_label(line: str, entities: pd.DataFrame):\n",
    "    line_set = line.replace(\",\", \"\").strip().split()\n",
    "    for i, column in enumerate(entities):\n",
    "        entity_values = entities.iloc[0, i].replace(\",\", \"\").strip()\n",
    "        entity_set = entity_values.split()\n",
    "        \n",
    "        \n",
    "        matches_count = 0\n",
    "        for l in line_set:\n",
    "            if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n",
    "                matches_count += 1\n",
    "            \n",
    "            if (column.upper() == 'ADDRESS' and (matches_count / len(line_set)) >= 0.5) or \\\n",
    "               (column.upper() != 'ADDRESS' and (matches_count == len(line_set))) or \\\n",
    "               matches_count == len(entity_set):\n",
    "                return column.upper()\n",
    "\n",
    "    return \"O\"\n",
    "\n",
    "\n",
    "line = bbox.loc[1,\"line\"]\n",
    "label = assign_line_label(line, entities)\n",
    "print(\"Line:\", line)\n",
    "print(\"Assigned label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(words: pd.DataFrame, entities: pd.DataFrame):\n",
    "    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n",
    "    already_labeled = {\"TOTAL\": False,\n",
    "                       \"DATE\": False,\n",
    "                       \"ADDRESS\": False,\n",
    "                       \"COMPANY\": False,\n",
    "                       \"O\": False\n",
    "    }\n",
    "\n",
    "    # Go through every line in $words and assign it a label\n",
    "    labels = []\n",
    "    for i, line in enumerate(words['line']):\n",
    "        label = assign_line_label(line, entities)\n",
    "\n",
    "        already_labeled[label] = True\n",
    "        if (label == \"ADDRESS\" and already_labeled[\"TOTAL\"]) or \\\n",
    "           (label == \"COMPANY\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"])):\n",
    "            label = \"O\"\n",
    "\n",
    "        # Assign to the largest bounding box\n",
    "        if label in [\"TOTAL\", \"DATE\"]:\n",
    "            x0_loc = words.columns.get_loc(\"x0\")\n",
    "            bbox = words.iloc[i, x0_loc:x0_loc+4].to_list()\n",
    "            area = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
    "\n",
    "            if max_area[label][0] < area:\n",
    "                max_area[label] = (area, i)\n",
    "\n",
    "            label = \"O\"\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    labels[max_area[\"DATE\"][1]] = \"DATE\"\n",
    "    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n",
    "\n",
    "    words[\"label\"] = labels\n",
    "    return words\n",
    "\n",
    "\n",
    "# Example usage\n",
    "bbox_labeled = assign_labels(bbox, entities)\n",
    "bbox_labeled.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOICE\n",
      "\n",
      "Cyt Carpe tt\n",
      "20) RE Rare\n",
      "fe te\n",
      "\n",
      "om & me)\n",
      "\n",
      "aon wn he\n",
      "\n",
      "Tt Beare AND Laer Somers oo\n",
      "ae etn terns Canw on A\n",
      "\n",
      "ire wee ye 41D ome vat toe wd\n",
      "\n",
      "’ fre tnt Com ene ¢ tr & timate «ante\n",
      "\n",
      ". me oe oth arn\n",
      "\n",
      "> \\arew Wee\n",
      "\n",
      "Shank gou\n",
      "\n",
      "es Uatt\n",
      "\n",
      "on met\n",
      "\n",
      "Wit 6 eee\n",
      "\n",
      "a etanele keene etal Roara’\n",
      "\n",
      "were a\n",
      "Se ees Fhe\n",
      "nsng FHF\n",
      "\n",
      "tte\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def split_line(line: pd.Series):\n",
    "  line_copy = line.copy()\n",
    "\n",
    "  line_str = line_copy.loc[\"line\"]\n",
    "  words = line_str.split(\" \")\n",
    "\n",
    "  # Filter unwanted tokens\n",
    "  words = [word for word in words if len(word) >= 1]\n",
    "\n",
    "  x0, y0, x2, y2 = line_copy.loc[['x0', 'y0', 'x2', 'y2']]\n",
    "  bbox_width = x2 - x0\n",
    "  \n",
    "\n",
    "  new_lines = []\n",
    "  for index, word in enumerate(words):\n",
    "    x2 = x0 + int(bbox_width * len(word)/len(line_str))\n",
    "    line_copy.at['x0', 'x2', 'line'] = [x0, x2, word]\n",
    "    new_lines.append(line_copy.to_list())\n",
    "    x0 = x2 + 5 \n",
    "\n",
    "  return new_lines\n",
    "\n",
    "\n",
    "# Example usage\n",
    "new_lines = split_line(bbox_labeled.loc[1])\n",
    "print(\"Original row:\")\n",
    "display(bbox_labeled.loc[1:1,:])\n",
    "\n",
    "print(\"Splitted row:\")\n",
    "pd.DataFrame(new_lines, columns=bbox_labeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "def dataset_creator(folder: Path):\n",
    "  bbox_folder = folder / 'box'\n",
    "  entities_folder = folder / 'entities'\n",
    "  img_folder = folder / 'img'\n",
    "\n",
    "  # Sort by filename so that when zipping them together\n",
    "  # we don't get some other file (just in case)\n",
    "  entities_files = sorted(entities_folder.glob(\"*.txt\"))\n",
    "  bbox_files = sorted(bbox_folder.glob(\"*.txt\"))\n",
    "  img_files = sorted(img_folder.glob(\"*.jpg\"))\n",
    "\n",
    "  data = []\n",
    "\n",
    "  print(\"Reading dataset:\")\n",
    "  for bbox_file, entities_file, img_file in tqdm(zip(bbox_files, entities_files, img_files), total=len(bbox_files)):            \n",
    "    # Read the files\n",
    "    bbox = read_bbox_and_words(bbox_file)\n",
    "    entities = read_entities(entities_file)\n",
    "    image = Image.open(img_file)\n",
    "\n",
    "    # Assign labels to lines in bbox using entities\n",
    "    bbox_labeled = assign_labels(bbox, entities)\n",
    "    del bbox\n",
    "\n",
    "    # Split lines into separate tokens\n",
    "    new_bbox_l = []\n",
    "    for index, row in bbox_labeled.iterrows():\n",
    "      new_bbox_l += split_line(row)\n",
    "    new_bbox = pd.DataFrame(new_bbox_l, columns=bbox_labeled.columns, dtype=np.int16)\n",
    "    del bbox_labeled\n",
    "\n",
    "\n",
    "    # Do another label assignment to keep the labeling more precise \n",
    "    for index, row in new_bbox.iterrows():\n",
    "      label = row['label']\n",
    "\n",
    "      if label != \"O\":\n",
    "        entity_values = entities.iloc[0, entities.columns.get_loc(label.lower())]\n",
    "        entity_set = entity_values.split()\n",
    "        \n",
    "        if any(SequenceMatcher(a=row['line'], b=b).ratio() > 0.7 for b in entity_set):\n",
    "            label = \"S-\" + label\n",
    "        else:\n",
    "            label = \"O\"\n",
    "      \n",
    "      new_bbox.at[index, 'label'] = label\n",
    "\n",
    "    width, height = image.size\n",
    "  \n",
    "    data.append([new_bbox, width, height])\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_creator(sroie_folder_path / 'train')\n",
    "dataset_test = dataset_creator(sroie_folder_path / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(points: list, width: int, height: int) -> list:\n",
    "  x0, y0, x2, y2 = [int(p) for p in points]\n",
    "  \n",
    "  x0 = int(1000 * (x0 / width))\n",
    "  x2 = int(1000 * (x2 / width))\n",
    "  y0 = int(1000 * (y0 / height))\n",
    "  y2 = int(1000 * (y2 / height))\n",
    "\n",
    "  return [x0, y0, x2, y2]\n",
    "\n",
    "\n",
    "def write_dataset(dataset: list, output_dir: Path, name: str):\n",
    "  print(f\"Writing {name}ing dataset:\")\n",
    "  with open(output_dir / f\"{name}.txt\", \"w+\", encoding=\"utf8\") as file, \\\n",
    "       open(output_dir / f\"{name}_box.txt\", \"w+\", encoding=\"utf8\") as file_bbox, \\\n",
    "       open(output_dir / f\"{name}_image.txt\", \"w+\", encoding=\"utf8\") as file_image:\n",
    "\n",
    "      # Go through each dataset\n",
    "      for datas in tqdm(dataset, total=len(dataset)):\n",
    "        data, width, height = datas\n",
    "        \n",
    "        filename = data.iloc[0, data.columns.get_loc('filename')]\n",
    "\n",
    "        # Go through every row in dataset\n",
    "        for index, row in data.iterrows():\n",
    "          bbox = [int(p) for p in row[['x0', 'y0', 'x2', 'y2']]]\n",
    "          normalized_bbox = normalize(bbox, width, height)\n",
    "\n",
    "          file.write(\"{}\\t{}\\n\".format(row['line'], row['label']))\n",
    "          file_bbox.write(\"{}\\t{} {} {} {}\\n\".format(row['line'], *normalized_bbox))\n",
    "          file_image.write(\"{}\\t{} {} {} {}\\t{} {}\\t{}\\n\".format(row['line'], *bbox, width, height, filename))\n",
    "\n",
    "        # Write a second newline to separate dataset from others\n",
    "        file.write(\"\\n\")\n",
    "        file_bbox.write(\"\\n\")\n",
    "        file_image.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = Path('/kaggle/working','dataset')\n",
    "\n",
    "dataset_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_dataset(dataset_train, dataset_directory, 'train')\n",
    "write_dataset(dataset_test, dataset_directory, 'test')\n",
    "\n",
    "# Creating the 'labels.txt' file to the the model what categories to predict.\n",
    "labels = ['COMPANY', 'DATE', 'ADDRESS', 'TOTAL']\n",
    "IOB_tags = ['S']\n",
    "with open(dataset_directory / 'labels.txt', 'w') as f:\n",
    "  for tag in IOB_tags:\n",
    "    for label in labels:\n",
    "      f.write(f\"{tag}-{label}\\n\")\n",
    "  # Writes in the last label O - meant for all non labeled words\n",
    "  f.write(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/microsoft/unilm.git\n",
    "cd unilm/layoutlm/deprecated\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_folder_input= sroie_folder_path / Path('layoutlm-base-uncased') # Define it so we can copy it into our working directory\n",
    "\n",
    "pretrained_model_folder=Path('/kaggle/working/layoutlm-base-uncased/') \n",
    "label_file=Path(dataset_directory, \"labels.txt\")\n",
    "\n",
    "# Move to the script directory\n",
    "os.chdir(\"/kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r \"{pretrained_model_folder_input}\" \"{pretrained_model_folder}\"\n",
    "! sed -i 's/\"num_attention_heads\": 16,/\"num_attention_heads\": 12,/' \"{pretrained_model_folder}/\"config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat \"/kaggle/working/layoutlm-base-uncased/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /kaggle/working/dataset/cached*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python run_seq_labeling.py \\\n",
    "                            --data_dir /kaggle/working/dataset \\\n",
    "                            --labels /kaggle/working/dataset/labels.txt \\\n",
    "                            --model_name_or_path \"{pretrained_model_folder}\" \\\n",
    "                            --model_type layoutlm \\\n",
    "                            --max_seq_length 512 \\\n",
    "                            --do_lower_case \\\n",
    "                            --do_train \\\n",
    "                            --num_train_epochs 10 \\\n",
    "                            --logging_steps 50 \\\n",
    "                            --save_steps -1 \\\n",
    "                            --output_dir output \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --per_gpu_train_batch_size 8 \\\n",
    "                            --per_gpu_eval_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for test set and make predictions\n",
    "! python run_seq_labeling.py \\\n",
    "                            --data_dir /kaggle/working/dataset \\\n",
    "                            --labels /kaggle/working/dataset/labels.txt \\\n",
    "                            --model_name_or_path \"{pretrained_model_folder}\" \\\n",
    "                            --model_type layoutlm \\\n",
    "                            --do_lower_case \\\n",
    "                            --max_seq_length 512 \\\n",
    "                            --do_predict \\\n",
    "                            --logging_steps 10 \\\n",
    "                            --save_steps -1 \\\n",
    "                            --output_dir output \\\n",
    "                            --per_gpu_eval_batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat output/test_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot, patches\n",
    "import matplotlib\n",
    "\n",
    "data = pd.read_csv(\"/kaggle/working/dataset/test_image.txt\", delimiter=\"\\t\", names=[\"name\", \"bbox\", \"size\", \"image\"])\n",
    "data_category = pd.read_csv(\"/kaggle/working/dataset/test.txt\", delimiter=\"\\t\", names=[\"name\", \"true_category\"]).drop(columns=[\"name\"])\n",
    "data_prediction_category = pd.read_csv(\"output/test_predictions.txt\", delimiter=\" \", names=[\"name\", \"prediction_category\"]).drop(columns=[\"name\"])\n",
    "\n",
    "data_merge = data.merge(data_category, left_index=True, right_index=True)\n",
    "merged = data_merge.merge(data_prediction_category, left_index=True, right_index=True)\n",
    "merged_groups = list(merged.groupby(\"image\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(data, file):\n",
    "  colors = {\n",
    "      \"S-TOTAL\": (255,0,0),\n",
    "      \"S-DATE\": (0,255,0),\n",
    "      \"S-ADDRESS\": (0,0, 255),\n",
    "      \"S-COMPANY\": (255,255,0),\n",
    "      \"O\": (192,192,192)\n",
    "  }\n",
    "\n",
    "\n",
    "  imagename = data[0].split(\".\")[0] + \".jpg\"\n",
    "  print(\"Filename:\",imagename)\n",
    "  image_path = str(sroie_folder_path / 'test' / 'img' / imagename)\n",
    "\n",
    "  img=cv2.imread(image_path)\n",
    "  img_prediction=cv2.imread(image_path)\n",
    "\n",
    "  data = data[1]\n",
    "  for bbox, category, prediction_category in zip(data['bbox'], data['true_category'], data['prediction_category']):\n",
    "    (x1, y1, x2, y2) = [int(coordinate) for coordinate in bbox.split()]\n",
    "\n",
    "    img_prediction = cv2.rectangle(img_prediction, (x1, y1), (x2, y2), colors[prediction_category], 2 if \"O\" in prediction_category else 4)\n",
    "    img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[category], 2 if \"O\" in category else 4)\n",
    "\n",
    "  matplotlib.rcParams['figure.figsize'] = 15 ,18\n",
    "\n",
    "  cv2.imwrite(\"prediction.jpg\", img_prediction)\n",
    "\n",
    "  # Plot\n",
    "  fig, ax = matplotlib.pyplot.subplots(1,2)\n",
    "  ax[0].set_title(\"Original\", fontsize= 30)\n",
    "  ax[0].imshow(img);\n",
    "  ax[1].set_title(\"Prediction\", fontsize= 30)\n",
    "  ax[1].imshow(img_prediction);\n",
    "\n",
    "  # Legend\n",
    "  handles = [\n",
    "      patches.Patch(color='yellow', label='Company'),\n",
    "      patches.Patch(color='blue', label='Address'),\n",
    "      patches.Patch(color='green', label='Date'),\n",
    "      patches.Patch(color='red', label='Total'),\n",
    "      patches.Patch(color='gray', label='Other')\n",
    "  ]\n",
    "\n",
    "  fig.legend(handles=handles, prop={'size': 25}, loc='lower center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(merged_groups[0], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(merged_groups[34], 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
